#!/usr/bin/env python3
"""
Qwen3-Omni è§†é¢‘æƒ…æ„Ÿåˆ†ææµ‹è¯•è„šæœ¬
ä½¿ç”¨Thinkeræ¨¡å‹åˆ†æè§†é¢‘ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾
"""

import os
import sys
import logging
import torch
from pathlib import Path
import glob

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_video_emotion_analysis():
    """æµ‹è¯•Qwen3-Omniå¯¹è§†é¢‘çš„æƒ…æ„Ÿåˆ†æèƒ½åŠ›"""
    
    try:
        # æ­¥éª¤1: æ£€æŸ¥ç¯å¢ƒ
        logger.info("æ­¥éª¤1: æ£€æŸ¥ç¯å¢ƒ...")
        logger.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        logger.info(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.info(f"GPUåç§°: {torch.cuda.get_device_name()}")
        
        # æ­¥éª¤2: å¯¼å…¥æ¨¡å—
        logger.info("æ­¥éª¤2: å¯¼å…¥Qwen3-Omniæ¨¡å—...")
        from transformers import Qwen3OmniMoeThinkerForConditionalGeneration, Qwen3OmniMoeProcessor
        from qwen_omni_utils import process_mm_info
        logger.info("âœ“ æˆåŠŸå¯¼å…¥Qwen3-Omniæ¨¡å—")
        
        # æ­¥éª¤3: è®¾ç½®è·¯å¾„
        model_path = os.path.expanduser("~/models/Qwen/Qwen3-Omni-30B-A3B-Instruct")
        video_dir = "/data/testmllm/project/verl/video_capture/videos"
        
        # è·å–ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶è¿›è¡Œæµ‹è¯•
        video_files = glob.glob(os.path.join(video_dir, "*.mp4"))
        if not video_files:
            raise FileNotFoundError(f"åœ¨ {video_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        
        test_video = video_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶
        logger.info(f"æµ‹è¯•è§†é¢‘: {test_video}")
        
        # æ­¥éª¤4: åŠ è½½å¤„ç†å™¨
        logger.info("æ­¥éª¤4: åŠ è½½å¤„ç†å™¨...")
        processor = Qwen3OmniMoeProcessor.from_pretrained(model_path)
        logger.info("âœ“ å¤„ç†å™¨åŠ è½½æˆåŠŸ")
        
        # æ­¥éª¤5: åŠ è½½æ¨¡å‹
        logger.info("æ­¥éª¤5: åŠ è½½Thinkeræ¨¡å‹åˆ°GPU...")
        model = Qwen3OmniMoeThinkerForConditionalGeneration.from_pretrained(
            model_path,
            dtype="auto",
            device_map="auto",
            trust_remote_code=True,
        )
        logger.info("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        logger.info(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
        
        # æ­¥éª¤6: å‡†å¤‡è§†é¢‘æƒ…æ„Ÿåˆ†æå¯¹è¯
        logger.info("æ­¥éª¤6: å‡†å¤‡è§†é¢‘æƒ…æ„Ÿåˆ†æ...")
        
        conversations = [
            {
                "role": "system",
                "content": [
                    {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech. You are an expert at analyzing emotions and facial expressions in videos."}
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "video", "video": test_video},
                    {"type": "text", "text": "è¯·åˆ†æè¿™ä¸ªè§†é¢‘ä¸­äººç‰©çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚è¯·è¯¦ç»†æè¿°ä½ è§‚å¯Ÿåˆ°çš„é¢éƒ¨è¡¨æƒ…ã€æƒ…ç»ªçŠ¶æ€ï¼Œä»¥åŠå¯èƒ½çš„æƒ…æ„Ÿå˜åŒ–ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
                ],
            },
        ]
        
        # æ­¥éª¤7: å¤„ç†è¾“å…¥
        logger.info("æ­¥éª¤7: å¤„ç†å¤šæ¨¡æ€è¾“å…¥...")
        inputs = processor.apply_chat_template(
            conversations,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
            padding=True,
        ).to(model.device)
        
        logger.info("âœ“ è¾“å…¥å¤„ç†å®Œæˆ")
        logger.info(f"è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
        
        # æ­¥éª¤8: ç”Ÿæˆæƒ…æ„Ÿåˆ†æç»“æœ
        logger.info("æ­¥éª¤8: å¼€å§‹è§†é¢‘æƒ…æ„Ÿåˆ†æ...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                do_sample=True,
                max_new_tokens=500,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id,
            )
        
        # æ­¥éª¤9: è§£ç ç»“æœ
        logger.info("æ­¥éª¤9: è§£ç åˆ†æç»“æœ...")
        
        # æå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        input_length = inputs['input_ids'].shape[1]
        new_tokens = outputs[:, input_length:]
        emotion_analysis = processor.batch_decode(new_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        
        logger.info("=" * 60)
        logger.info("ğŸ­ è§†é¢‘æƒ…æ„Ÿåˆ†æç»“æœ:")
        logger.info("=" * 60)
        logger.info(f"æµ‹è¯•è§†é¢‘: {os.path.basename(test_video)}")
        logger.info(f"åˆ†æç»“æœ:\n{emotion_analysis}")
        logger.info("=" * 60)
        
        # æ­¥éª¤10: æµ‹è¯•å¤šä¸ªè§†é¢‘ï¼ˆå¯é€‰ï¼‰
        if len(video_files) > 1:
            logger.info(f"æ­¥éª¤10: æµ‹è¯•æ›´å¤šè§†é¢‘æ–‡ä»¶ (å…±{len(video_files)}ä¸ª)...")
            
            # æµ‹è¯•å‰3ä¸ªè§†é¢‘
            for i, video_file in enumerate(video_files[1:4], 1):
                logger.info(f"åˆ†æç¬¬{i+1}ä¸ªè§†é¢‘: {os.path.basename(video_file)}")
                
                conversations_multi = [
                    {
                        "role": "system",
                        "content": [
                            {"type": "text", "text": "You are an expert at analyzing emotions in videos. Please provide concise emotion analysis."}
                        ],
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "video", "video": video_file},
                            {"type": "text", "text": "è¯·ç®€è¦åˆ†æè¿™ä¸ªè§†é¢‘ä¸­çš„ä¸»è¦æƒ…æ„Ÿè¡¨è¾¾ï¼Œç”¨1~3ä¸ªæƒ…æ„Ÿè¯è¯­æ¦‚æ‹¬ã€‚"},
                        ],
                    },
                ]
                
                inputs_multi = processor.apply_chat_template(
                    conversations_multi,
                    add_generation_prompt=True,
                    tokenize=True,
                    return_dict=True,
                    return_tensors="pt",
                    padding=True,
                ).to(model.device)
                
                with torch.no_grad():
                    outputs_multi = model.generate(
                        **inputs_multi,
                        do_sample=True,
                        max_new_tokens=150,
                        temperature=0.7,
                        pad_token_id=processor.tokenizer.eos_token_id,
                    )
                
                input_length_multi = inputs_multi['input_ids'].shape[1]
                new_tokens_multi = outputs_multi[:, input_length_multi:]
                result_multi = processor.batch_decode(new_tokens_multi, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
                
                logger.info(f"è§†é¢‘{i+1}åˆ†æ: {result_multi.strip()}")
        
        logger.info("ğŸ‰ è§†é¢‘æƒ…æ„Ÿåˆ†ææµ‹è¯•æˆåŠŸå®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        return False

if __name__ == "__main__":
    success = test_video_emotion_analysis()
    sys.exit(0 if success else 1)